{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9df27621-ab4d-4a64-b922-81bb35f6f537",
   "metadata": {},
   "source": [
    "# AH ASSESSMENT | DATA ENGINEERING | TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "020fccab-b1aa-40a9-8f09-e4602dda36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "265499e4-4cc7-4cd8-ad96-2db1f8bb0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,lit,to_date,regexp_replace,regexp_extract, expr\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType,DateType\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3abd7f9-e2a3-49a4-9f0c-85fefee17c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49931f0-55ab-4535-bffa-b0ebfa060f93",
   "metadata": {},
   "source": [
    "### INITIALISE SPARK SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaae2e98-5111-49a6-ab2f-9e72c2ee97fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/nncube1/Library/Python/3.10/lib/python/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/nncube1/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/nncube1/.ivy2/jars\n",
      "com.crealytics#spark-excel_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4680e424-cbf4-4493-97a5-145dcfc56b81;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.crealytics#spark-excel_2.12;0.13.7 in central\n",
      "\tfound org.apache.poi#poi;4.1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.13 in central\n",
      "\tfound org.apache.commons#commons-collections4;4.4 in central\n",
      "\tfound org.apache.commons#commons-math3;3.6.1 in central\n",
      "\tfound com.zaxxer#SparseBitSet;1.2 in central\n",
      "\tfound org.apache.poi#poi-ooxml;4.1.2 in central\n",
      "\tfound org.apache.poi#poi-ooxml-schemas;4.1.2 in central\n",
      "\tfound org.apache.xmlbeans#xmlbeans;3.1.0 in central\n",
      "\tfound com.github.virtuald#curvesapi;1.06 in central\n",
      "\tfound com.norbitltd#spoiwo_2.12;1.8.0 in central\n",
      "\tfound org.scala-lang.modules#scala-xml_2.12;1.3.0 in central\n",
      "\tfound com.github.pjfanning#excel-streaming-reader;2.3.6 in central\n",
      "\tfound com.github.pjfanning#poi-shared-strings;1.0.4 in central\n",
      "\tfound com.h2database#h2;1.4.200 in central\n",
      "\tfound org.apache.commons#commons-text;1.8 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.9 in central\n",
      "\tfound xml-apis#xml-apis;1.4.01 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.commons#commons-compress;1.20 in central\n",
      ":: resolution report :: resolve 1803ms :: artifacts dl 63ms\n",
      "\t:: modules in use:\n",
      "\tcom.crealytics#spark-excel_2.12;0.13.7 from central in [default]\n",
      "\tcom.github.pjfanning#excel-streaming-reader;2.3.6 from central in [default]\n",
      "\tcom.github.pjfanning#poi-shared-strings;1.0.4 from central in [default]\n",
      "\tcom.github.virtuald#curvesapi;1.06 from central in [default]\n",
      "\tcom.h2database#h2;1.4.200 from central in [default]\n",
      "\tcom.norbitltd#spoiwo_2.12;1.8.0 from central in [default]\n",
      "\tcom.zaxxer#SparseBitSet;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.13 from central in [default]\n",
      "\torg.apache.commons#commons-collections4;4.4 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.20 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.9 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.8 from central in [default]\n",
      "\torg.apache.poi#poi;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-ooxml;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-ooxml-schemas;4.1.2 from central in [default]\n",
      "\torg.apache.xmlbeans#xmlbeans;3.1.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-xml_2.12;1.3.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\txml-apis#xml-apis;1.4.01 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.commons#commons-compress;1.19 by [org.apache.commons#commons-compress;1.20] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   0   |   0   |   1   ||   20  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4680e424-cbf4-4493-97a5-145dcfc56b81\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 20 already retrieved (0kB/34ms)\n",
      "23/10/11 23:29:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName('AH_ASSESSMENT_TRANSFORM').config(\"spark.jars.packages\",\"com.crealytics:spark-excel_2.12:0.13.7\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97327398-d507-495c-ae6f-64646b575e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.148:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AH_ASSESSMENT_TRANSFORM</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1164d5780>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea8c30b-7c83-498e-9a20-c1ca3f7c9561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PYSPARK\n",
    "spark.conf.set(\"spark.sql.legacy.allowUntypedScalaUDF\", \"true\")  # Equivalent to pd.set_option('mode.chained_assignment', None)\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", \"true\")  # Equivalent to pd.set_option('display.max_rows', None)\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.maxNumRows\", 100)  # Equivalent to pd.set_option('display.max_columns', None)\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 0)\n",
    "spark.conf.set(\"zeppelin.spark.printREPLOutput\", \"true\")# Equivalent to pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "#PANDAS\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658ead0d-1047-4679-ba1e-c050407fd484",
   "metadata": {},
   "source": [
    "### INGESTING THE FILE FROM THE RAW DATA ZONE FOR TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d040e386-9911-48fe-9281-fd8e26b66b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THE 'GET FILES' MODULE.THIS IS SIMILAR TO IMPORT\n",
    "%run \"UTILS/files.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eadd1b9-f8ef-4637-9f82-09760fd8b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINING LOCATIONS FOR FILE\n",
    "source='1.RAW_DATA_LAYER'\n",
    "destination='2.TRANSFORMATION_LAYER'\n",
    "artefact_file_location='4.ARTEFACTS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef7d6759-f178-4724-a26b-ad793566206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully read modelling (1).xlsx from 1.RAW_DATA_LAYER/modelling (1).xlsx.\n"
     ]
    }
   ],
   "source": [
    "df,file_name=get_files(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3373844c-c566-4fd7-98e4-c5600e07ad95",
   "metadata": {},
   "source": [
    "### DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f12e37-f671-4e2a-bc2f-1ead46a3c353",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Sorting out dates | visit_start_date,visit_end_date,pet_dob,dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e00e165-f0a5-4b92-b43b-23856def02dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:DATES SUCCESSFULLY PROCESSED\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"visit_start_date\", to_date(col(\"visit_start_date\"), 'dd/MM/yyyy')) \\\n",
    "               .withColumn(\"visit_end_date\", to_date(col(\"visit_end_date\"), 'dd/MM/yyyy')) \\\n",
    "               .withColumn(\"pet_dob\", to_date(col(\"pet_dob\"), 'dd/MM/yyyy')) \\\n",
    "               .withColumn(\"dt\",lit(datetime.now().strftime(\"%Y%m%d\"))) # adding partition date column based on today\n",
    "\n",
    "logger.info(f\"DATES SUCCESSFULLY PROCESSED\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485aae8a-98f9-4be0-94ce-3c95ea4b85d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Sorting out currency | visit_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7124de8b-5634-4fc0-b3ee-6894b3792373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove non-numeric characters from the visit_cost column and it two decimal places\n",
    "\n",
    "df = df.withColumn(\"visit_cost_numeric\", expr(\"CAST(regexp_replace(visit_cost, '[^0-9.]', '') AS DECIMAL(10,2))\")) \\\n",
    "                  .withColumn(\"currency\", regexp_extract(col(\"visit_cost\"), r'[^\\d.]+', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81c6628f-63bf-4063-b5ec-f0c7f2eb25f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>currency</th><th>count</th></tr>\n",
       "<tr><td>$</td><td>11</td></tr>\n",
       "<tr><td>£</td><td>989</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+-----+\n",
       "|currency|count|\n",
       "+--------+-----+\n",
       "|$       |11   |\n",
       "|£       |989  |\n",
       "+--------+-----+"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See distribution of currency\n",
    "count_per_currency = df.groupBy(\"currency\").count()\n",
    "count_per_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d25d72d-e954-457a-b36d-268f71b12174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' QUESTION FOR BUSINESS,WAS THIS PAID WITH FOREIGN CURRENCY OR IS IT A MISTAKE?\\n\\n    IF FOREIGN CURRENCY:\\n        WE NEED TO CONVERT IT FROM $ TO £\\n    IF MISTAKE,WE NEED TO REPLACE $ WITH £ \\n    \\n    Since cities seem made up and $ make up 1.1% of the data,I cannot verify this\\n    For the sake of this assessment,I am just going to assume $ were a mistake'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' QUESTION FOR BUSINESS,WAS THIS PAID WITH FOREIGN CURRENCY OR IS IT A MISTAKE?\n",
    "\n",
    "    IF FOREIGN CURRENCY:\n",
    "        WE NEED TO CONVERT IT FROM $ TO £\n",
    "    IF MISTAKE,WE NEED TO REPLACE $ WITH £ \n",
    "    \n",
    "    Since cities seem made up and $ make up 1.1% of the data,I cannot verify this\n",
    "    For the sake of this assessment,I am just going to assume $ were a mistake'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e25eb0fa-15e9-41b2-ad57-6cbb7a90cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace $ with £\n",
    "df = df.withColumn(\"currency\", regexp_replace(col(\"currency\"), \"\\\\$\", \"£\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d3766d5-1744-4d96-96df-8329505f72e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>currency</th><th>count</th></tr>\n",
       "<tr><td>£</td><td>1000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+-----+\n",
       "|currency|count|\n",
       "+--------+-----+\n",
       "|£       |1000 |\n",
       "+--------+-----+"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirm removal of $\n",
    "count_per_currency = df.groupBy(\"currency\").count()\n",
    "count_per_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2252e941-609e-4a34-bbdd-72dd1c69370a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:VISIT_COST SUCCESSFULLY PROCESSED\n"
     ]
    }
   ],
   "source": [
    "# Drop visit_cost to replace it with the version without currency symbols\n",
    "df=df.drop(\"visit_cost\")\n",
    "df = df.withColumnRenamed(\"visit_cost_numeric\", \"visit_cost\")\n",
    "\n",
    "logger.info(f\"VISIT_COST SUCCESSFULLY PROCESSED\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412b7ca-88d6-4eae-b0ab-d7cb5c91cdda",
   "metadata": {},
   "source": [
    "### DEFINE DESIRED SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dd33e10-3284-4abe-90f9-6d3ba1756443",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_schema = StructType([\n",
    "    StructField(\"visit_id\", IntegerType(), True),\n",
    "    StructField(\"visit_start_date\", DateType(), True),\n",
    "    StructField(\"visit_end_date\", DateType(), True),\n",
    "    StructField(\"visit_cost\", DoubleType(), True),\n",
    "    StructField(\"currency\", StringType(), True),\n",
    "    StructField(\"procedure_code\", StringType(), True),\n",
    "    StructField(\"procedure_desc\", StringType(), True),\n",
    "    StructField(\"hospital_id\", IntegerType(), True),\n",
    "    StructField(\"hospital\", StringType(), True),\n",
    "    StructField(\"owner_id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"postcode\", StringType(), True),\n",
    "    StructField(\"pet_id\", IntegerType(), True),\n",
    "    StructField(\"pet_name\", StringType(), True),\n",
    "    StructField(\"species\", StringType(), True),\n",
    "    StructField(\"breed\", StringType(), True),\n",
    "    StructField(\"pet_dob\", DateType(), True),\n",
    "    StructField(\"dt\", StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f49617eb-7f4e-4486-9c49-a26852e53267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each field,cast data type based on custom schema\n",
    "for field in custom_schema.fields:\n",
    "    df = df.withColumn(field.name, col(field.name).cast(field.dataType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "156beb2c-9c76-4ee1-a1d9-a16932f19378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:SCHEMA CHECK SUCCESSFULLY COMPLETED\n"
     ]
    }
   ],
   "source": [
    "# Check if the DataFrame schema matches the predefined schema\n",
    "if set(df.schema.fields) != set(custom_schema.fields):\n",
    "    raise ValueError(\"DataFrame schema does not match the defined schema.\")\n",
    "    \n",
    "    \n",
    "logger.info(f\"SCHEMA CHECK SUCCESSFULLY COMPLETED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cb00e58-64ea-4f36-8028-290c3bb27f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select desired fields and enforce order\n",
    "\n",
    "df = df.select(\"visit_id\", \"visit_start_date\", \"visit_end_date\", \"currency\",\"visit_cost\", \"procedure_code\", \"procedure_desc\", \"hospital_id\", \"hospital\", \"owner_id\", \"first_name\", \"last_name\", \"email\", \"address\", \"city\", \"postcode\", \"pet_id\", \"pet_name\", \"species\", \"breed\", \"pet_dob\",\"dt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed635c4e-23a6-4f8c-8f6c-f19a4fe504fd",
   "metadata": {},
   "source": [
    "### DATA QUALITY CHECKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a6684-4eb7-4f16-a8fd-41c3107b6010",
   "metadata": {},
   "source": [
    "##### 1. Duplicate Records \n",
    "- Removing any duplicates records in the data\n",
    "- If found,save number of duplicates to text file for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9d0ad8a-2a21-4841-8b34-fe54d5d9d6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:DUPLICATE CHECK SUCCESSFULLY COMPLETED                            \n"
     ]
    }
   ],
   "source": [
    "duplicate_counts = df.groupBy(df.columns).count().filter('count > 1')\n",
    "duplicates_removed=duplicate_counts.count()\n",
    "\n",
    "if duplicates_removed != 0:\n",
    "    duplicate_output_path = f\"{artefact_file_location}/Duplicates_for_{file_name}.txt\"\n",
    "    print(duplicate_output_path)\n",
    "    \n",
    "    # Saving the count of duplicates to a text file for future reference\n",
    "    try:\n",
    "        with open(duplicate_output_path, 'w') as file:\n",
    "            file.write(f\"Number of Duplicates Removed: {duplicates_removed}\")\n",
    "            logger.info(f\"Number of Duplicated removed saved to {duplicate_output_path}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "\n",
    "    # Deleting duplicates from df\n",
    "    df = df.dropDuplicates()\n",
    "    logger.info(f\"Duplicates removed\")\n",
    "else:\n",
    "    logger.info(f\"DUPLICATE CHECK SUCCESSFULLY COMPLETED\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bcbdf8-61d6-4022-8ad2-13f20a7f6cdd",
   "metadata": {},
   "source": [
    "##### 2. Invalid emails \n",
    "- Identify invalid emails\n",
    "- If found,write records to a path in the artefacts folder\n",
    "- save no of invalid emails found to a variable for later use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9843e89d-b5bf-4800-833a-0463af3a7c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:INVALID EMAILS CHECK SUCCESSFULLY COMPLETED                       \n"
     ]
    }
   ],
   "source": [
    "df_invalid_emails = df.filter((df[\"email\"].contains(\"@\") == False) & (df[\"email\"] != \"\")).distinct()\n",
    "invalid_emails_path = f\"{artefact_file_location}/Invalid_emails_for_{file_name}.csv\"\n",
    "if df_invalid_emails.count() !=0:\n",
    "    try:\n",
    "        df_invalid_emails.coalesce(1).write.mode(\"overwrite\").csv(invalid_emails_path)\n",
    "        no_of_invalid_emails=df_invalid_emails.count()\n",
    "        logger.info(f\"{no_of_invalid_emails} invalid emails found.See {invalid_emails_path} for records found\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "else:\n",
    "    no_of_invalid_emails=df_invalid_emails.count()\n",
    "    logger.info(f\"INVALID EMAILS CHECK SUCCESSFULLY COMPLETED\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c8eef3-3905-49e5-abc2-340d21de8d0c",
   "metadata": {},
   "source": [
    "#### 2. Summary stats for data quality \n",
    "\n",
    "- Work out no of missing values,min and max,data types and no of invalid emails per column\n",
    "- Save as a csv table to visualise later.The viz will also calculate the data quality score per column and overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2297a7b6-19c5-45d9-ad6b-c3d73b34c320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pandas_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c19a2451-d32f-4f34-a8ee-5ed69899c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_records = pandas_df.shape\n",
    "# print(total_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c0e93bd-8cc3-4835-b594-f09f46f0107e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/pw84ghpj123_713ztc7q6vdc0000gp/T/ipykernel_22733/3688974351.py:9: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  min_values = pandas_df.min().reset_index()\n",
      "/var/folders/d4/pw84ghpj123_713ztc7q6vdc0000gp/T/ipykernel_22733/3688974351.py:14: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  max_values = pandas_df.max().reset_index()\n",
      "INFO:__main__:Successfully written Data Quality Summary Table to '4.ARTEFACTS/quality_stats_for_modelling (1).xlsx.csv'.\n",
      "INFO:__main__:DATA QUALITY CHECKS SUCCESSFULLY COMPLETED\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate missing values minimum and maximum values for each column\n",
    "\n",
    "#-----------------------MISSING VALUES-------------------------------------\n",
    "missing_values = pandas_df.isnull().sum().reset_index()\n",
    "missing_values.columns = ['field', 'missing_values']\n",
    "\n",
    "#-----------------------MIN VALUES-------------------------------------\n",
    "\n",
    "min_values = pandas_df.min().reset_index()\n",
    "min_values.columns = ['field', 'min']\n",
    "\n",
    "#-----------------------MAX VALUES-------------------------------------\n",
    "\n",
    "max_values = pandas_df.max().reset_index()\n",
    "max_values.columns = ['field', 'max']\n",
    "\n",
    "#-----------------------DATA TYPES-------------------------------------\n",
    "\n",
    "\n",
    "data_types = pandas_df.dtypes.reset_index()\n",
    "data_types.columns = ['field', 'data_type']\n",
    "\n",
    "\n",
    "# Merge the DataFrames on the 'field' column\n",
    "result_df = pd.merge(data_types,missing_values, on='field')\n",
    "result_df = pd.merge(result_df, min_values, on='field')\n",
    "result_df = pd.merge(result_df, max_values, on='field')\n",
    "\n",
    "#-----------------------INVALID EMAILS-------------------------------------\n",
    "result_df[\"invalid\"] = np.where(result_df[\"field\"] == \"email\",no_of_invalid_emails, \"\")\n",
    "\n",
    "\n",
    "quality_stats_path = f\"{artefact_file_location}/quality_stats_for_{file_name}.csv\"\n",
    "\n",
    "try:\n",
    "    result_df.to_csv(quality_stats_path, index=False,sep=',')\n",
    "    logger.info(f\"Successfully written Data Quality Summary Table to '{quality_stats_path}'.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "result_df\n",
    "\n",
    "logger.info(f\"DATA QUALITY CHECKS SUCCESSFULLY COMPLETED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab7c44e-737b-4360-8934-02c68fe2c35c",
   "metadata": {},
   "source": [
    "### DIMENSIONAL MODELLING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f163e4-2f0e-44dc-b0ff-23110e8f0088",
   "metadata": {},
   "source": [
    "#### 1. Create a dimensional model \n",
    "\n",
    "- Create dfs for each dimension based on the original df\n",
    "- Create a fact table by joining the dimension tables with the original DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5696a5ee-2348-4c90-b183-948c9bea7c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:DIMENSION MODEL CREATED'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visit Dimension:\n",
      "root\n",
      " |-- visit_id: integer (nullable = true)\n",
      " |-- visit_start_date: date (nullable = true)\n",
      " |-- visit_end_date: date (nullable = true)\n",
      " |-- visit_cost: double (nullable = true)\n",
      " |-- procedure_code: string (nullable = true)\n",
      " |-- hospital_id: integer (nullable = true)\n",
      " |-- owner_id: integer (nullable = true)\n",
      " |-- pet_id: integer (nullable = true)\n",
      " |-- dt: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    visits = df.select(\n",
    "        \"visit_id\",\n",
    "        col(\"visit_start_date\").alias(\"visit_start_date_v\"),  # Alias the column to avoid naming conflicts\n",
    "        col(\"visit_end_date\").alias(\"visit_end_date_v\"), \n",
    "        col(\"currency\"),\n",
    "        col(\"visit_cost\").alias(\"visit_cost_v\"),\n",
    "        col(\"procedure_code\").alias(\"procedure_code_v\"),\n",
    "        col(\"hospital_id\").alias(\"hospital_id_v\"),\n",
    "        col(\"owner_id\").alias(\"owner_id_v\"),\n",
    "        col(\"pet_id\").alias(\"pet_id_v\"),\n",
    "        col(\"dt\").alias(\"dt_v\")\n",
    "    ).distinct()\n",
    "\n",
    "    # Create dimension tables for Procedure, Hospital, Owner, and Pet. Alias for the column 'dt' to avoid naming conflicts\n",
    "    procedures = df.select(\"procedure_code\", \"procedure_desc\", col(\"dt\").alias(\"dt_p\")).distinct()\n",
    "    hospitals = df.select(\"hospital_id\", \"hospital\",col(\"dt\").alias(\"dt_h\")).distinct()\n",
    "    owners = df.select(\"owner_id\", \"first_name\", \"last_name\", \"email\", \"address\", \"city\", \"postcode\",col(\"dt\").alias(\"dt_o\")).distinct()\n",
    "    pets = df.select(\"pet_id\", \"pet_name\", \"species\", \"breed\", \"pet_dob\",col(\"dt\").alias(\"dt_pt\")).distinct()\n",
    "\n",
    "    # Create fact table by joining the dimension tables with the original DataFrame\n",
    "    fact_table = df.join(visits, \"visit_id\", \"inner\") \\\n",
    "        .join(procedures, \"procedure_code\", \"inner\") \\\n",
    "        .join(hospitals, \"hospital_id\", \"inner\") \\\n",
    "        .join(owners, \"owner_id\", \"inner\") \\\n",
    "        .join(pets, \"pet_id\", \"inner\") \\\n",
    "        .select(\n",
    "            \"visit_id\",\n",
    "            \"visit_start_date\",\n",
    "            \"visit_end_date\",\n",
    "            \"visit_cost\",\n",
    "            \"procedure_code\",\n",
    "            \"hospital_id\",\n",
    "            \"owner_id\",\n",
    "            \"pet_id\",\n",
    "            \"dt\"\n",
    "        )\n",
    "    logger.info(f\"DIMENSION MODEL CREATED'.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "\n",
    "# Show the resulting DataFrames\n",
    "print(\"Visit Dimension:\")\n",
    "fact_table.printSchema()\n",
    "\n",
    "# print(\"Procedure Dimension:\")\n",
    "# procedures\n",
    "\n",
    "# print(\"Hospital Dimension:\")\n",
    "# hospitals\n",
    "\n",
    "# print(\"Owner Dimension:\")\n",
    "# owners\n",
    "\n",
    "# print(\"Pet Dimension:\")\n",
    "# pets\n",
    "\n",
    "# print(\"Fact Table:\")\n",
    "# fact_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36039995-3be2-4feb-8ea9-f1920913c882",
   "metadata": {},
   "source": [
    "#### 2. Rename the columns for cleaner tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9395bc13-b17e-4eeb-b2bc-b838488a791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visits = df.select(\n",
    "    \"visit_id\",\n",
    "    col(\"visit_start_date\").alias(\"visit_start_date\"),  # Remove \"_v\"\n",
    "    col(\"visit_end_date\").alias(\"visit_end_date\"), \n",
    "    col(\"currency\"),\n",
    "    col(\"visit_cost\").alias(\"visit_cost\"),\n",
    "    col(\"procedure_code\").alias(\"procedure_code\"),\n",
    "    col(\"hospital_id\").alias(\"hospital_id\"),\n",
    "    col(\"owner_id\").alias(\"owner_id\"),\n",
    "    col(\"pet_id\").alias(\"pet_id\"),\n",
    "    col(\"dt\").alias(\"dt\")\n",
    ").distinct()\n",
    "\n",
    "\n",
    "procedures = procedures.withColumnRenamed(\"dt_p\", \"dt\")\n",
    "hospitals = hospitals.withColumnRenamed(\"dt_h\", \"dt\")\n",
    "pets = pets.withColumnRenamed(\"dt_pt\", \"dt\")\n",
    "owners = owners.withColumnRenamed(\"dt_o\", \"dt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b03e0d-1c5f-46a2-aa6f-a5bdde7fcc13",
   "metadata": {},
   "source": [
    "##### 3. Write into the transformation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e903c7bf-62ce-498c-b693-a54c6409413b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully written visits to '2.TRANSFORMATION_LAYER/visits/dt=20231011'.\n",
      "INFO:__main__:Successfully written procedures to '2.TRANSFORMATION_LAYER/procedures/dt=20231011'.\n",
      "INFO:__main__:Successfully written hospitals to '2.TRANSFORMATION_LAYER/hospitals/dt=20231011'.\n",
      "INFO:__main__:Successfully written owners to '2.TRANSFORMATION_LAYER/owners/dt=20231011'.\n",
      "INFO:__main__:Successfully written pets to '2.TRANSFORMATION_LAYER/pets/dt=20231011'.\n",
      "INFO:__main__:Successfully written fact_table to '2.TRANSFORMATION_LAYER/fact_table/dt=20231011'.\n"
     ]
    }
   ],
   "source": [
    "tables = ['visits', 'procedures', 'hospitals', 'owners', 'pets', 'fact_table']\n",
    "\n",
    "for table in tables:\n",
    "    parquet_folder_path = f'{destination}/{table}'\n",
    "    today_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "    today_partition_path = f'{parquet_folder_path}/dt={today_date}'\n",
    "    try:\n",
    "        visits.write.mode(\"overwrite\").parquet(today_partition_path)\n",
    "        logger.info(f\"Successfully written {table} to '{today_partition_path}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d31b5b-2d59-4030-9c83-ec46af1fdb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
